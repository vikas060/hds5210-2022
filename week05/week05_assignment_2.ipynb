{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 5 Exercises\n",
    "\n",
    "_McKinney 3.2_\n",
    "\n",
    "**Unlike in previous weeks, in the exercises below, you will need to create a function definition from scratch.  I'll provide specific instructions and examples for how it will be used, but you will have to do the work of definiging the whole function.**\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "longest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 25.1 Longest String\n",
    "\n",
    "Write a function called `longest (L)` that takes as its only parameter a list of strings.  Your function needs to find the longest of those strings and return the position number of that longest string.  An example is provided below:\n",
    "\n",
    "The longest word in that list of strings is \"birthday\" in position #1, so `longest(strings)` should return 1.\n",
    "```\n",
    ">>> strings = ['happy', 'birthday', 'to', 'me']\n",
    ">>> longest(strings)\n",
    "1\n",
    "```\n",
    "\n",
    "Be sure to include a docstring.  Include test cases in your docstring if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "longest-answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "def longest(strings):\n",
    "    \"\"\"(list) -> int\n",
    "    This function finds the longest string in the list of input strings\n",
    "    and returns the position number of that longest string.\n",
    "    \n",
    "    >>> longest(['happy', 'birthday', 'to', 'me'])\n",
    "    1\n",
    "    \n",
    "    >>> longest(['one', 'two', 'three', 'four'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    \n",
    "    longest_pos = 0\n",
    "    longest_len = 0\n",
    "    \n",
    "    print(\"Position  Item         Longest So Far\")\n",
    "    print(\"--------- ------------ --------------\")\n",
    "    for position, item in enumerate(strings):\n",
    "        \n",
    "        if len(item) > longest_len:\n",
    "            longest_pos = position\n",
    "            longest_len = len(item)\n",
    "        print(\"{0:9} {1:12} {2} : {3}\".format(position, item, longest_pos, longest_len))\n",
    "            \n",
    "    return longest_pos\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest(my_list):\n",
    "    \"\"\"(list)->int\n",
    "    Find the longest item in a list of strings and return the index for that string.\n",
    "    If there are multiple strings with the same length, return the index of the first of those longest strings.\n",
    "    \n",
    "    >>> longest(['happy', 'birthday', 'to', 'me'])\n",
    "    1\n",
    "    \n",
    "    >>> longest(['abc','def','ghi'])\n",
    "    0\n",
    "    \"\"\"\n",
    "\n",
    "    if type(my_list) != list:\n",
    "        raise TypeError(\"That isn't a list!\")\n",
    "    \n",
    "    if len(my_list) == 0:\n",
    "        raise ValueError(\"There were no items in that list!\")\n",
    "    \n",
    "    max_length = -1\n",
    "    max_index = None\n",
    "\n",
    "    for num, item in enumerate(my_list):\n",
    "        try:\n",
    "            if len(item) > max_length:\n",
    "                max_length = len(item)\n",
    "                max_index = num\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return max_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest(['happy', 'birthday', 2, 'me'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest(['happy', 'birthday', 'to', 'me'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "longest-tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert longest(\"happy birthday to me\".split(\" \")) == 1\n",
    "assert longest(\"enjoy class\".split(\" \")) == 0\n",
    "assert longest(['when','what','where','how','who']) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    longest(['happy', 'birthday', 'to', 'me'])\n",
      "Expecting:\n",
      "    1\n",
      "ok\n",
      "Trying:\n",
      "    longest(['abc','def','ghi'])\n",
      "Expecting:\n",
      "    0\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.run_docstring_examples(longest, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.2 Celsius to Farenheit\n",
    "\n",
    "Write a function called f_to_c() that converts a given temperature in degrees Farenheit to degrees Celsius.  If you don't recall that conversion, it is:\n",
    "\n",
    "$ temp_c = \\frac{5}{9} \\times (temp_f - 32) $\n",
    "\n",
    "Make sure that you code is well documented using the DocString examples, and that your code includes tests for 212f, 32f, and 98.6f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def f_to_c(F):\n",
    "    \"\"\"(float) -> float\n",
    "    The function that converts a given temperature in degrees fahrenheit to degree celsius.\n",
    "    \n",
    "    >>> f_to_c(212)\n",
    "    100.0\n",
    "    \n",
    "    >>>f_to_c(32)\n",
    "    0.0\n",
    "    \n",
    "    >>>f_to_c(98.6)\n",
    "    37.0\n",
    "    \"\"\"\n",
    "    \n",
    "    c = (5/9) * (F - 32)\n",
    "    return c\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert f_to_c(212) == 100.0\n",
    "assert f_to_c(32) == 0.0\n",
    "assert f_to_c(98.6) == 37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_to_c(212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "line 7 of the docstring for NoName lacks blank after >>>: '    >>>f_to_c(32)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-10b0d48899d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdoctest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoctest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_docstring_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_to_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36mrun_docstring_examples\u001b[0;34m(f, globs, verbose, name, compileflags, optionflags)\u001b[0m\n\u001b[1;32m   2104\u001b[0m     \u001b[0mfinder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocTestFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocTestRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptionflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptionflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2106\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2107\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompileflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompileflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, obj, name, module, globs, extraglobs)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0;31m# Recursively explore `obj`, extracting DocTests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mtests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[0;31m# Sort the tests by alpha order of names, for consistency in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;31m# verbose-mode output.  This was a feature of doctest in Pythons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36m_find\u001b[0;34m(self, tests, obj, name, module, source_lines, globs, seen)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;31m# Find a test for this object, and add it to the list of tests.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36m_get_test\u001b[0;34m(self, obj, name, module, globs, source_lines)\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         return self._parser.get_doctest(docstring, globs, name,\n\u001b[0;32m-> 1067\u001b[0;31m                                         filename, lineno)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_find_lineno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36mget_doctest\u001b[0;34m(self, string, globs, name, filename, lineno)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \"\"\"\n\u001b[0;32m--> 669\u001b[0;31m         return DocTest(self.get_examples(string, name), globs,\n\u001b[0m\u001b[1;32m    670\u001b[0m                        name, filename, lineno, string)\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36mget_examples\u001b[0;34m(self, string, name)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0monly\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0merror\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \"\"\"\n\u001b[0;32m--> 683\u001b[0;31m         return [x for x in self.parse(string, name)\n\u001b[0m\u001b[1;32m    684\u001b[0m                 if isinstance(x, Example)]\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, string, name)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;31m# Extract info from the regexp match.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m             \u001b[0;31m# Create an Example, and add it to the list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IS_BLANK_OR_COMMENT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36m_parse_example\u001b[0;34m(self, m, name, lineno)\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;31m# indented; and then strip their indentation & prompts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0msource_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_prompt_blank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_lines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mindent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msource_lines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.6/doctest.py\u001b[0m in \u001b[0;36m_check_prompt_blank\u001b[0;34m(self, lines, indent, name, lineno)\u001b[0m\n\u001b[1;32m    788\u001b[0m                                  \u001b[0;34m'lacks blank after %s: %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                                  (lineno+i+1, name,\n\u001b[0;32m--> 790\u001b[0;31m                                   line[indent:indent+3], line))\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: line 7 of the docstring for NoName lacks blank after >>>: '    >>>f_to_c(32)'"
     ]
    }
   ],
   "source": [
    "import doctest\n",
    "doctest.run_docstring_examples(f_to_c, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25.3 Computing Length of Stay\n",
    "\n",
    "For this problem, we have a collection of patient enounter data stored as a Python dictionary.  The `key` for the dictionary is the **encounter ID**, a code that starts with the letter `E` followed by four numbers.  The value associated with each encounter ID is another Python dictionary.  This \"inner\" dictionary holds three items: admit date, primary diagnosis, and discharge date.  See the example in the code below.\n",
    "\n",
    "You need to write a length of stay function that computes the length of stay, in whole days, between the admit date and discharge date.  However, if the diagnosis is \"Observation\" then the length of stay should always be returned as 0 regardless of the admit and discharge dates.\n",
    "\n",
    "You will find it handy to refer to this example here on how to calculate the number of days between two dates: https://stackoverflow.com/questions/151199/how-to-calculate-number-of-days-between-two-given-dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "def los(ad,dis,diag):\n",
    "    \"\"\"\n",
    "    the los function takes admit, discharge and diagnosis type as input, then calculate the number of days from Amit to discharge using date module. if condition is used to check if the diagnosis is observation. If thatâ€™s the case, it returns 0. Else it returns the number of days.\n",
    "    \"\"\"\n",
    "    delta =  (dis - ad )\n",
    "    days = delta.days\n",
    "    if(diag == \"Observation\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return days\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.run_docstring_examples(los, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters = { \n",
    "    \"E1234\": { \"admit\": date(2019,1,3), \"diagnosis\": \"COPD\", \"discharge\": date(2019,1,8) },\n",
    "    \"E8342\": { \"admit\": date(2019,1,5), \"diagnosis\": \"Hypertension\", \"discharge\": date(2019,1,9) },\n",
    "    \"E9231\": { \"admit\": date(2019,1,12), \"diagnosis\": \"Anxiety\", \"discharge\": date(2019,1,13) },\n",
    "    \"E8333\": { \"admit\": date(2019,1,15), \"diagnosis\": \"Observation\", \"discharge\": date(2019,1,16) },\n",
    "    \"E3342\": { \"admit\": date(2019,1,4), \"diagnosis\": \"Anxiety\", \"discharge\": date(2019,1,4)}\n",
    "}\n",
    "\n",
    "\n",
    "for e, v in encounters.items():\n",
    "    print(los(v['admit'],v['discharge'],v['diagnosis']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### 25.4 Average Length of Stay\n",
    "\n",
    "Create a function called `average_los` that returns the average of the LOS for the encounters in the provided dictionary. The encounters must be a dictionary that contains dictionaries that each contain at least an admit date and a discharge date.\n",
    "    \n",
    "If the length of stay of any individual encounter is 0, then it will not be counted toward the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = {\n",
    "        \"E1234\": { \"admit\": date(2019,1,3), \"diagnosis\": \"COPD\", \"discharge\": date(2019,1,8) }\n",
    "}\n",
    "\n",
    "test2 = {\n",
    "        \"E1234\": { \"admit\": date(2019,1,3), \"diagnosis\": \"COPD\", \"discharge\": date(2019,1,8) }\n",
    "}\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "def average_los(test):\n",
    "    \"\"\"\n",
    "    The average_los takes test dictionary which contains admit, diagnosis and discharge, and uses if statement to check the following condition:\n",
    "    If the number of days between admit and discharge is 0, then it is not added to the total average, else it is added to the total average and it is returned.\n",
    "    \"\"\"\n",
    "     \n",
    "    s = 0\n",
    "    count = 0\n",
    "    avg = 0.0\n",
    "    for e,v in encounters.items():\n",
    "        d = (v[\"discharge\"] - v[\"admit\"]).days\n",
    "        if(d > 0):\n",
    "            s = s + d\n",
    "            count = count + 1\n",
    "    avg = s / count\n",
    "    return avg\n",
    "    \"\"\" \n",
    "    >>> average_los(test1)\n",
    "    5\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "encounters = { \n",
    "    \"E1234\": { \"admit\": date(2019,1,3), \"diagnosis\": \"COPD\", \"discharge\": date(2019,1,8) },\n",
    "    \"E8342\": { \"admit\": date(2019,1,5), \"diagnosis\": \"Hypertension\", \"discharge\": date(2019,1,9) },\n",
    "    \"E9231\": { \"admit\": date(2019,1,12), \"diagnosis\": \"Anxiety\", \"discharge\": date(2019,1,13) },\n",
    "    \"E8333\": { \"admit\": date(2019,1,15), \"diagnosis\": \"Observation\", \"discharge\": date(2019,1,16) },\n",
    "    \"E3342\": { \"admit\": date(2019,1,4), \"diagnosis\": \"Anxiety\", \"discharge\": date(2019,1,4)}\n",
    "}\n",
    "\n",
    "assert(average_los(encounters)) == 2.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.run_docstring_examples(average_los, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 25.5 Celsius to Farenheit\n",
    "\n",
    "Write your own function called `c_to_f` that converts degrees Celsius to degrees Farenheit.  Include in your solution a series of doc tests that can verify the conversion using inputs of 100, 0, and 37 degrees Celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def c_to_f(c):\n",
    "    def f_to_c():\n",
    "       \"\"\"\n",
    "\n",
    "       \"\"\"\n",
    "    f = 1.8 * c + 32\n",
    "    return f\n",
    "    print(c_to_f(100))\n",
    "    print(c_to_f(0))\n",
    "    print(c_to_f(37))\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "doctest.run_docstring_examples(c_to_f, globals(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Check your work above\n",
    "\n",
    "If you didn't get them all correct, take a few minutes to think through those that aren't correct.\n",
    "\n",
    "\n",
    "## Submitting Your Work\n",
    "\n",
    "In order to submit your work, you'll need to use the `git` command line program to **add** your homework file (this file) to your local repository, **commit** your changes to your local repository, and then **push** those changes up to github.com.  From there, I'll be able to **pull** the changes down and do my grading.  I'll provide some feedback, **commit** and **push** my comments back to you.  Next week, I'll show you how to **pull** down my comments.\n",
    "\n",
    "To run through everything one last time and submit your work:\n",
    "1. Use the `Kernel` -> `Restart Kernel and Run All Cells` menu option to run everything from top to bottom and stop here.\n",
    "2. Follow the instruction on the prompt below to either ssave and submit your work, or continue working.\n",
    "\n",
    "If anything fails along the way with this submission part of the process, let me know.  I'll help you troubleshoort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "a=input('''\n",
    "Are you ready to submit your work?\n",
    "1. Click the Save icon (or do Ctrl-S / Cmd-S)\n",
    "2. Type \"yes\" or \"no\" below\n",
    "3. Press Enter\n",
    "\n",
    "''')\n",
    "\n",
    "if a=='yes':\n",
    "    !git add week05_assignment_2.ipynb\n",
    "    !git commit -a -m \"Submitting the week 5 programming exercises\"\n",
    "    !git push\n",
    "else:\n",
    "    print('''\n",
    "    \n",
    "OK. We can wait.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "If the message above says something like _Submitting the week 3 review exercises_ or _Everything is up to date_, then your work was submitted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
